"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class Extent(google.protobuf.message.Message):
    """Data is packed into blocks on disk, always starting from the beginning
    of the block. If a file's data is too large for one block, it overflows
    into another block, which may or may not be the following block on the
    physical partition. An ordered list of extents is another
    representation of an ordered list of blocks. For example, a file stored
    in blocks 9, 10, 11, 2, 18, 12 (in that order) would be stored in
    extents { {9, 3}, {2, 1}, {18, 1}, {12, 1} } (in that order).
    In general, files are stored sequentially on disk, so it's more efficient
    to use extents to encode the block lists (this is effectively
    run-length encoding).
    A sentinel value (kuint64max) as the start block denotes a sparse-hole
    in a file whose block-length is specified by num_blocks.

    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    START_BLOCK_FIELD_NUMBER: builtins.int
    NUM_BLOCKS_FIELD_NUMBER: builtins.int
    start_block: builtins.int
    num_blocks: builtins.int
    def __init__(self,
        *,
        start_block: typing.Optional[builtins.int] = ...,
        num_blocks: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["num_blocks",b"num_blocks","start_block",b"start_block"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["num_blocks",b"num_blocks","start_block",b"start_block"]) -> None: ...
global___Extent = Extent

class Signatures(google.protobuf.message.Message):
    """Signatures: Updates may be signed by the OS vendor. The client verifies
    an update's signature by hashing the entire download. The section of the
    download that contains the signature is at the end of the file, so when
    signing a file, only the part up to the signature part is signed.
    Then, the client looks inside the download's Signatures message for a
    Signature message that it knows how to handle. Generally, a client will
    only know how to handle one type of signature, but an update may contain
    many signatures to support many different types of client. Then client
    selects a Signature message and uses that, along with a known public key,
    to verify the download. The public key is expected to be part of the
    client.

    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class Signature(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor
        VERSION_FIELD_NUMBER: builtins.int
        DATA_FIELD_NUMBER: builtins.int
        UNPADDED_SIGNATURE_SIZE_FIELD_NUMBER: builtins.int
        version: builtins.int
        data: builtins.bytes
        unpadded_signature_size: builtins.int
        """The DER encoded signature size of EC keys is nondeterministic for
        different input of sha256 hash. However, we need the size of the
        serialized signatures protobuf string to be fixed before signing;
        because this size is part of the content to be signed. Therefore, we
        always pad the signature data to the maximum possible signature size of
        a given key. And the payload verifier will truncate the signature to
        its correct size based on the value of |unpadded_signature_size|.
        """

        def __init__(self,
            *,
            version: typing.Optional[builtins.int] = ...,
            data: typing.Optional[builtins.bytes] = ...,
            unpadded_signature_size: typing.Optional[builtins.int] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["data",b"data","unpadded_signature_size",b"unpadded_signature_size","version",b"version"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["data",b"data","unpadded_signature_size",b"unpadded_signature_size","version",b"version"]) -> None: ...

    SIGNATURES_FIELD_NUMBER: builtins.int
    @property
    def signatures(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Signatures.Signature]: ...
    def __init__(self,
        *,
        signatures: typing.Optional[typing.Iterable[global___Signatures.Signature]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["signatures",b"signatures"]) -> None: ...
global___Signatures = Signatures

class PartitionInfo(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    SIZE_FIELD_NUMBER: builtins.int
    HASH_FIELD_NUMBER: builtins.int
    size: builtins.int
    hash: builtins.bytes
    def __init__(self,
        *,
        size: typing.Optional[builtins.int] = ...,
        hash: typing.Optional[builtins.bytes] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["hash",b"hash","size",b"size"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["hash",b"hash","size",b"size"]) -> None: ...
global___PartitionInfo = PartitionInfo

class InstallOperation(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _Type:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[InstallOperation._Type.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        REPLACE: InstallOperation._Type.ValueType  # 0
        """Replace destination extents w/ attached data."""

        REPLACE_BZ: InstallOperation._Type.ValueType  # 1
        """Replace destination extents w/ attached bzipped data."""

        MOVE: InstallOperation._Type.ValueType  # 2
        """Move source extents to target extents."""

        BSDIFF: InstallOperation._Type.ValueType  # 3
        """The data is a bsdiff binary diff."""

        SOURCE_COPY: InstallOperation._Type.ValueType  # 4
        """On minor version 2 or newer, these operations are supported:
        Copy from source to target partition
        """

        SOURCE_BSDIFF: InstallOperation._Type.ValueType  # 5
        """Like BSDIFF, but read from source partition"""

        REPLACE_XZ: InstallOperation._Type.ValueType  # 8
        """On minor version 3 or newer and on major version 2 or newer, these
        operations are supported:
        Replace destination extents w/ attached xz data.
        """

        ZERO: InstallOperation._Type.ValueType  # 6
        """On minor version 4 or newer, these operations are supported:
        Write zeros in the destination.
        """

        DISCARD: InstallOperation._Type.ValueType  # 7
        """Discard the destination blocks, reading as undefined."""

        BROTLI_BSDIFF: InstallOperation._Type.ValueType  # 10
        """Like SOURCE_BSDIFF, but compressed with brotli."""

        PUFFDIFF: InstallOperation._Type.ValueType  # 9
        """On minor version 5 or newer, these operations are supported:
        The data is in puffdiff format.
        """

        ZUCCHINI: InstallOperation._Type.ValueType  # 11
        """On minor version 8 or newer, these operations are supported:"""

        LZ4DIFF_BSDIFF: InstallOperation._Type.ValueType  # 12
        """On minor version 9 or newer, these operations are supported:"""

        LZ4DIFF_PUFFDIFF: InstallOperation._Type.ValueType  # 13
    class Type(_Type, metaclass=_TypeEnumTypeWrapper):
        pass

    REPLACE: InstallOperation.Type.ValueType  # 0
    """Replace destination extents w/ attached data."""

    REPLACE_BZ: InstallOperation.Type.ValueType  # 1
    """Replace destination extents w/ attached bzipped data."""

    MOVE: InstallOperation.Type.ValueType  # 2
    """Move source extents to target extents."""

    BSDIFF: InstallOperation.Type.ValueType  # 3
    """The data is a bsdiff binary diff."""

    SOURCE_COPY: InstallOperation.Type.ValueType  # 4
    """On minor version 2 or newer, these operations are supported:
    Copy from source to target partition
    """

    SOURCE_BSDIFF: InstallOperation.Type.ValueType  # 5
    """Like BSDIFF, but read from source partition"""

    REPLACE_XZ: InstallOperation.Type.ValueType  # 8
    """On minor version 3 or newer and on major version 2 or newer, these
    operations are supported:
    Replace destination extents w/ attached xz data.
    """

    ZERO: InstallOperation.Type.ValueType  # 6
    """On minor version 4 or newer, these operations are supported:
    Write zeros in the destination.
    """

    DISCARD: InstallOperation.Type.ValueType  # 7
    """Discard the destination blocks, reading as undefined."""

    BROTLI_BSDIFF: InstallOperation.Type.ValueType  # 10
    """Like SOURCE_BSDIFF, but compressed with brotli."""

    PUFFDIFF: InstallOperation.Type.ValueType  # 9
    """On minor version 5 or newer, these operations are supported:
    The data is in puffdiff format.
    """

    ZUCCHINI: InstallOperation.Type.ValueType  # 11
    """On minor version 8 or newer, these operations are supported:"""

    LZ4DIFF_BSDIFF: InstallOperation.Type.ValueType  # 12
    """On minor version 9 or newer, these operations are supported:"""

    LZ4DIFF_PUFFDIFF: InstallOperation.Type.ValueType  # 13

    TYPE_FIELD_NUMBER: builtins.int
    DATA_OFFSET_FIELD_NUMBER: builtins.int
    DATA_LENGTH_FIELD_NUMBER: builtins.int
    SRC_EXTENTS_FIELD_NUMBER: builtins.int
    SRC_LENGTH_FIELD_NUMBER: builtins.int
    DST_EXTENTS_FIELD_NUMBER: builtins.int
    DST_LENGTH_FIELD_NUMBER: builtins.int
    DATA_SHA256_HASH_FIELD_NUMBER: builtins.int
    SRC_SHA256_HASH_FIELD_NUMBER: builtins.int
    type: global___InstallOperation.Type.ValueType
    data_offset: builtins.int
    """Only minor version 6 or newer support 64 bits |data_offset| and
    |data_length|, older client will read them as uint32.
    The offset into the delta file (after the protobuf)
    where the data (if any) is stored
    """

    data_length: builtins.int
    """The length of the data in the delta file"""

    @property
    def src_extents(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Extent]:
        """Ordered list of extents that are read from (if any) and written to."""
        pass
    src_length: builtins.int
    """Byte length of src, equal to the number of blocks in src_extents *
    block_size. It is used for BSDIFF and SOURCE_BSDIFF, because we need to
    pass that external program the number of bytes to read from the blocks we
    pass it.  This is not used in any other operation.
    """

    @property
    def dst_extents(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Extent]: ...
    dst_length: builtins.int
    """Byte length of dst, equal to the number of blocks in dst_extents *
    block_size. Used for BSDIFF and SOURCE_BSDIFF, but not in any other
    operation.
    """

    data_sha256_hash: builtins.bytes
    """Optional SHA 256 hash of the blob associated with this operation.
    This is used as a primary validation for http-based downloads and
    as a defense-in-depth validation for https-based downloads. If
    the operation doesn't refer to any blob, this field will have
    zero bytes.
    """

    src_sha256_hash: builtins.bytes
    """Indicates the SHA 256 hash of the source data referenced in src_extents at
    the time of applying the operation. If present, the update_engine daemon
    MUST read and verify the source data before applying the operation.
    """

    def __init__(self,
        *,
        type: typing.Optional[global___InstallOperation.Type.ValueType] = ...,
        data_offset: typing.Optional[builtins.int] = ...,
        data_length: typing.Optional[builtins.int] = ...,
        src_extents: typing.Optional[typing.Iterable[global___Extent]] = ...,
        src_length: typing.Optional[builtins.int] = ...,
        dst_extents: typing.Optional[typing.Iterable[global___Extent]] = ...,
        dst_length: typing.Optional[builtins.int] = ...,
        data_sha256_hash: typing.Optional[builtins.bytes] = ...,
        src_sha256_hash: typing.Optional[builtins.bytes] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["data_length",b"data_length","data_offset",b"data_offset","data_sha256_hash",b"data_sha256_hash","dst_length",b"dst_length","src_length",b"src_length","src_sha256_hash",b"src_sha256_hash","type",b"type"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data_length",b"data_length","data_offset",b"data_offset","data_sha256_hash",b"data_sha256_hash","dst_extents",b"dst_extents","dst_length",b"dst_length","src_extents",b"src_extents","src_length",b"src_length","src_sha256_hash",b"src_sha256_hash","type",b"type"]) -> None: ...
global___InstallOperation = InstallOperation

class CowMergeOperation(google.protobuf.message.Message):
    """Hints to VAB snapshot to skip writing some blocks if these blocks are
    identical to the ones on the source image. The src & dst extents for each
    CowMergeOperation should be contiguous, and they're a subset of an OTA
    InstallOperation.
    During merge time, we need to follow the pre-computed sequence to avoid
    read after write, similar to the inplace update schema.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    class _Type:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[CowMergeOperation._Type.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        COW_COPY: CowMergeOperation._Type.ValueType  # 0
        """identical blocks"""

        COW_XOR: CowMergeOperation._Type.ValueType  # 1
        """used when src/dst blocks are highly similar"""

        COW_REPLACE: CowMergeOperation._Type.ValueType  # 2
        """Raw replace operation"""

    class Type(_Type, metaclass=_TypeEnumTypeWrapper):
        pass

    COW_COPY: CowMergeOperation.Type.ValueType  # 0
    """identical blocks"""

    COW_XOR: CowMergeOperation.Type.ValueType  # 1
    """used when src/dst blocks are highly similar"""

    COW_REPLACE: CowMergeOperation.Type.ValueType  # 2
    """Raw replace operation"""


    TYPE_FIELD_NUMBER: builtins.int
    SRC_EXTENT_FIELD_NUMBER: builtins.int
    DST_EXTENT_FIELD_NUMBER: builtins.int
    SRC_OFFSET_FIELD_NUMBER: builtins.int
    type: global___CowMergeOperation.Type.ValueType
    @property
    def src_extent(self) -> global___Extent: ...
    @property
    def dst_extent(self) -> global___Extent: ...
    src_offset: builtins.int
    """For COW_XOR, source location might be unaligned, so this field is in range
    [0, block_size), representing how much should the src_extent shift toward
    larger block number. If this field is non-zero, then src_extent will
    include 1 extra block in the end, as the merge op actually references the
    first |src_offset| bytes of that extra block. For example, if |dst_extent|
    is [10, 15], |src_offset| is 500, then src_extent might look like [25, 31].
    Note that |src_extent| contains 1 extra block than the |dst_extent|.
    """

    def __init__(self,
        *,
        type: typing.Optional[global___CowMergeOperation.Type.ValueType] = ...,
        src_extent: typing.Optional[global___Extent] = ...,
        dst_extent: typing.Optional[global___Extent] = ...,
        src_offset: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["dst_extent",b"dst_extent","src_extent",b"src_extent","src_offset",b"src_offset","type",b"type"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["dst_extent",b"dst_extent","src_extent",b"src_extent","src_offset",b"src_offset","type",b"type"]) -> None: ...
global___CowMergeOperation = CowMergeOperation

class PartitionUpdate(google.protobuf.message.Message):
    """Describes the update to apply to a single partition."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    PARTITION_NAME_FIELD_NUMBER: builtins.int
    RUN_POSTINSTALL_FIELD_NUMBER: builtins.int
    POSTINSTALL_PATH_FIELD_NUMBER: builtins.int
    FILESYSTEM_TYPE_FIELD_NUMBER: builtins.int
    NEW_PARTITION_SIGNATURE_FIELD_NUMBER: builtins.int
    OLD_PARTITION_INFO_FIELD_NUMBER: builtins.int
    NEW_PARTITION_INFO_FIELD_NUMBER: builtins.int
    OPERATIONS_FIELD_NUMBER: builtins.int
    POSTINSTALL_OPTIONAL_FIELD_NUMBER: builtins.int
    HASH_TREE_DATA_EXTENT_FIELD_NUMBER: builtins.int
    HASH_TREE_EXTENT_FIELD_NUMBER: builtins.int
    HASH_TREE_ALGORITHM_FIELD_NUMBER: builtins.int
    HASH_TREE_SALT_FIELD_NUMBER: builtins.int
    FEC_DATA_EXTENT_FIELD_NUMBER: builtins.int
    FEC_EXTENT_FIELD_NUMBER: builtins.int
    FEC_ROOTS_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    MERGE_OPERATIONS_FIELD_NUMBER: builtins.int
    ESTIMATE_COW_SIZE_FIELD_NUMBER: builtins.int
    partition_name: typing.Text
    """A platform-specific name to identify the partition set being updated. For
    example, in Chrome OS this could be "ROOT" or "KERNEL".
    """

    run_postinstall: builtins.bool
    """Whether this partition carries a filesystem with post-install program that
    must be run to finalize the update process. See also |postinstall_path| and
    |filesystem_type|.
    """

    postinstall_path: typing.Text
    """The path of the executable program to run during the post-install step,
    relative to the root of this filesystem. If not set, the default "postinst"
    will be used. This setting is only used when |run_postinstall| is set and
    true.
    """

    filesystem_type: typing.Text
    """The filesystem type as passed to the mount(2) syscall when mounting the new
    filesystem to run the post-install program. If not set, a fixed list of
    filesystems will be attempted. This setting is only used if
    |run_postinstall| is set and true.
    """

    @property
    def new_partition_signature(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Signatures.Signature]:
        """If present, a list of signatures of the new_partition_info.hash signed with
        different keys. If the update_engine daemon requires vendor-signed images
        and has its public key installed, one of the signatures should be valid
        for /postinstall to run.
        """
        pass
    @property
    def old_partition_info(self) -> global___PartitionInfo: ...
    @property
    def new_partition_info(self) -> global___PartitionInfo: ...
    @property
    def operations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___InstallOperation]:
        """The list of operations to be performed to apply this PartitionUpdate. The
        associated operation blobs (in operations[i].data_offset, data_length)
        should be stored contiguously and in the same order.
        """
        pass
    postinstall_optional: builtins.bool
    """Whether a failure in the postinstall step for this partition should be
    ignored.
    """

    @property
    def hash_tree_data_extent(self) -> global___Extent:
        """On minor version 6 or newer, these fields are supported:

        The extent for data covered by verity hash tree.
        """
        pass
    @property
    def hash_tree_extent(self) -> global___Extent:
        """The extent to store verity hash tree."""
        pass
    hash_tree_algorithm: typing.Text
    """The hash algorithm used in verity hash tree."""

    hash_tree_salt: builtins.bytes
    """The salt used for verity hash tree."""

    @property
    def fec_data_extent(self) -> global___Extent:
        """The extent for data covered by FEC."""
        pass
    @property
    def fec_extent(self) -> global___Extent:
        """The extent to store FEC."""
        pass
    fec_roots: builtins.int
    """The number of FEC roots."""

    version: typing.Text
    """Per-partition version used for downgrade detection, added
    as an effort to support partial updates. For most partitions,
    this is the build timestamp.
    """

    @property
    def merge_operations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___CowMergeOperation]:
        """A sorted list of CowMergeOperation. When writing cow, we can choose to
        skip writing the raw bytes for these extents. During snapshot merge, the
        bytes will read from the source partitions instead.
        """
        pass
    estimate_cow_size: builtins.int
    """Estimated size for COW image. This is used by libsnapshot
    as a hint. If set to 0, libsnapshot should use alternative
    methods for estimating size.
    """

    def __init__(self,
        *,
        partition_name: typing.Optional[typing.Text] = ...,
        run_postinstall: typing.Optional[builtins.bool] = ...,
        postinstall_path: typing.Optional[typing.Text] = ...,
        filesystem_type: typing.Optional[typing.Text] = ...,
        new_partition_signature: typing.Optional[typing.Iterable[global___Signatures.Signature]] = ...,
        old_partition_info: typing.Optional[global___PartitionInfo] = ...,
        new_partition_info: typing.Optional[global___PartitionInfo] = ...,
        operations: typing.Optional[typing.Iterable[global___InstallOperation]] = ...,
        postinstall_optional: typing.Optional[builtins.bool] = ...,
        hash_tree_data_extent: typing.Optional[global___Extent] = ...,
        hash_tree_extent: typing.Optional[global___Extent] = ...,
        hash_tree_algorithm: typing.Optional[typing.Text] = ...,
        hash_tree_salt: typing.Optional[builtins.bytes] = ...,
        fec_data_extent: typing.Optional[global___Extent] = ...,
        fec_extent: typing.Optional[global___Extent] = ...,
        fec_roots: typing.Optional[builtins.int] = ...,
        version: typing.Optional[typing.Text] = ...,
        merge_operations: typing.Optional[typing.Iterable[global___CowMergeOperation]] = ...,
        estimate_cow_size: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["estimate_cow_size",b"estimate_cow_size","fec_data_extent",b"fec_data_extent","fec_extent",b"fec_extent","fec_roots",b"fec_roots","filesystem_type",b"filesystem_type","hash_tree_algorithm",b"hash_tree_algorithm","hash_tree_data_extent",b"hash_tree_data_extent","hash_tree_extent",b"hash_tree_extent","hash_tree_salt",b"hash_tree_salt","new_partition_info",b"new_partition_info","old_partition_info",b"old_partition_info","partition_name",b"partition_name","postinstall_optional",b"postinstall_optional","postinstall_path",b"postinstall_path","run_postinstall",b"run_postinstall","version",b"version"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["estimate_cow_size",b"estimate_cow_size","fec_data_extent",b"fec_data_extent","fec_extent",b"fec_extent","fec_roots",b"fec_roots","filesystem_type",b"filesystem_type","hash_tree_algorithm",b"hash_tree_algorithm","hash_tree_data_extent",b"hash_tree_data_extent","hash_tree_extent",b"hash_tree_extent","hash_tree_salt",b"hash_tree_salt","merge_operations",b"merge_operations","new_partition_info",b"new_partition_info","new_partition_signature",b"new_partition_signature","old_partition_info",b"old_partition_info","operations",b"operations","partition_name",b"partition_name","postinstall_optional",b"postinstall_optional","postinstall_path",b"postinstall_path","run_postinstall",b"run_postinstall","version",b"version"]) -> None: ...
global___PartitionUpdate = PartitionUpdate

class DynamicPartitionGroup(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    NAME_FIELD_NUMBER: builtins.int
    SIZE_FIELD_NUMBER: builtins.int
    PARTITION_NAMES_FIELD_NUMBER: builtins.int
    name: typing.Text
    """Name of the group."""

    size: builtins.int
    """Maximum size of the group. The sum of sizes of all partitions in the group
    must not exceed the maximum size of the group.
    """

    @property
    def partition_names(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """A list of partitions that belong to the group."""
        pass
    def __init__(self,
        *,
        name: typing.Optional[typing.Text] = ...,
        size: typing.Optional[builtins.int] = ...,
        partition_names: typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["name",b"name","size",b"size"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","partition_names",b"partition_names","size",b"size"]) -> None: ...
global___DynamicPartitionGroup = DynamicPartitionGroup

class DynamicPartitionMetadata(google.protobuf.message.Message):
    """Metadata related to all dynamic partitions."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    GROUPS_FIELD_NUMBER: builtins.int
    SNAPSHOT_ENABLED_FIELD_NUMBER: builtins.int
    VABC_ENABLED_FIELD_NUMBER: builtins.int
    VABC_COMPRESSION_PARAM_FIELD_NUMBER: builtins.int
    COW_VERSION_FIELD_NUMBER: builtins.int
    @property
    def groups(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___DynamicPartitionGroup]:
        """All updatable groups present in |partitions| of this DeltaArchiveManifest.
        - If an updatable group is on the device but not in the manifest, it is
          not updated. Hence, the group will not be resized, and partitions cannot
          be added to or removed from the group.
        - If an updatable group is in the manifest but not on the device, the group
          is added to the device.
        """
        pass
    snapshot_enabled: builtins.bool
    """Whether dynamic partitions have snapshots during the update. If this is
    set to true, the update_engine daemon creates snapshots for all dynamic
    partitions if possible. If this is unset, the update_engine daemon MUST
    NOT create snapshots for dynamic partitions.
    """

    vabc_enabled: builtins.bool
    """If this is set to false, update_engine should not use VABC regardless. If
    this is set to true, update_engine may choose to use VABC if device
    supports it, but not guaranteed.
    VABC stands for Virtual AB Compression
    """

    vabc_compression_param: typing.Text
    """The compression algorithm used by VABC. Available ones are "gz", "brotli".
    See system/core/fs_mgr/libsnapshot/cow_writer.cpp for available options,
    as this parameter is ultimated forwarded to libsnapshot's CowWriter
    """

    cow_version: builtins.int
    """COW version used by VABC. The represents the major version in the COW
    header
    """

    def __init__(self,
        *,
        groups: typing.Optional[typing.Iterable[global___DynamicPartitionGroup]] = ...,
        snapshot_enabled: typing.Optional[builtins.bool] = ...,
        vabc_enabled: typing.Optional[builtins.bool] = ...,
        vabc_compression_param: typing.Optional[typing.Text] = ...,
        cow_version: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["cow_version",b"cow_version","snapshot_enabled",b"snapshot_enabled","vabc_compression_param",b"vabc_compression_param","vabc_enabled",b"vabc_enabled"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cow_version",b"cow_version","groups",b"groups","snapshot_enabled",b"snapshot_enabled","vabc_compression_param",b"vabc_compression_param","vabc_enabled",b"vabc_enabled"]) -> None: ...
global___DynamicPartitionMetadata = DynamicPartitionMetadata

class ApexInfo(google.protobuf.message.Message):
    """Definition has been duplicated from
    $ANDROID_BUILD_TOP/build/tools/releasetools/ota_metadata.proto. Keep in sync.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    PACKAGE_NAME_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    IS_COMPRESSED_FIELD_NUMBER: builtins.int
    DECOMPRESSED_SIZE_FIELD_NUMBER: builtins.int
    package_name: typing.Text
    version: builtins.int
    is_compressed: builtins.bool
    decompressed_size: builtins.int
    def __init__(self,
        *,
        package_name: typing.Optional[typing.Text] = ...,
        version: typing.Optional[builtins.int] = ...,
        is_compressed: typing.Optional[builtins.bool] = ...,
        decompressed_size: typing.Optional[builtins.int] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["decompressed_size",b"decompressed_size","is_compressed",b"is_compressed","package_name",b"package_name","version",b"version"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["decompressed_size",b"decompressed_size","is_compressed",b"is_compressed","package_name",b"package_name","version",b"version"]) -> None: ...
global___ApexInfo = ApexInfo

class ApexMetadata(google.protobuf.message.Message):
    """Definition has been duplicated from
    $ANDROID_BUILD_TOP/build/tools/releasetools/ota_metadata.proto. Keep in sync.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    APEX_INFO_FIELD_NUMBER: builtins.int
    @property
    def apex_info(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ApexInfo]: ...
    def __init__(self,
        *,
        apex_info: typing.Optional[typing.Iterable[global___ApexInfo]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["apex_info",b"apex_info"]) -> None: ...
global___ApexMetadata = ApexMetadata

class DeltaArchiveManifest(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor
    BLOCK_SIZE_FIELD_NUMBER: builtins.int
    SIGNATURES_OFFSET_FIELD_NUMBER: builtins.int
    SIGNATURES_SIZE_FIELD_NUMBER: builtins.int
    MINOR_VERSION_FIELD_NUMBER: builtins.int
    PARTITIONS_FIELD_NUMBER: builtins.int
    MAX_TIMESTAMP_FIELD_NUMBER: builtins.int
    DYNAMIC_PARTITION_METADATA_FIELD_NUMBER: builtins.int
    PARTIAL_UPDATE_FIELD_NUMBER: builtins.int
    APEX_INFO_FIELD_NUMBER: builtins.int
    block_size: builtins.int
    """(At time of writing) usually 4096"""

    signatures_offset: builtins.int
    """If signatures are present, the offset into the blobs, generally
    tacked onto the end of the file, and the length. We use an offset
    rather than a bool to allow for more flexibility in future file formats.
    If either is absent, it means signatures aren't supported in this
    file.
    """

    signatures_size: builtins.int
    minor_version: builtins.int
    """The minor version, also referred as "delta version", of the payload.
    Minor version 0 is full payload, everything else is delta payload.
    """

    @property
    def partitions(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___PartitionUpdate]:
        """Only present in major version >= 2. List of partitions that will be
        updated, in the order they will be updated. This field replaces the
        |install_operations|, |kernel_install_operations| and the
        |{old,new}_{kernel,rootfs}_info| fields used in major version = 1. This
        array can have more than two partitions if needed, and they are identified
        by the partition name.
        """
        pass
    max_timestamp: builtins.int
    """The maximum timestamp of the OS allowed to apply this payload.
    Can be used to prevent downgrading the OS.
    """

    @property
    def dynamic_partition_metadata(self) -> global___DynamicPartitionMetadata:
        """Metadata related to all dynamic partitions."""
        pass
    partial_update: builtins.bool
    """If the payload only updates a subset of partitions on the device."""

    @property
    def apex_info(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ApexInfo]:
        """Information on compressed APEX to figure out how much space is required for
        their decompression
        """
        pass
    def __init__(self,
        *,
        block_size: typing.Optional[builtins.int] = ...,
        signatures_offset: typing.Optional[builtins.int] = ...,
        signatures_size: typing.Optional[builtins.int] = ...,
        minor_version: typing.Optional[builtins.int] = ...,
        partitions: typing.Optional[typing.Iterable[global___PartitionUpdate]] = ...,
        max_timestamp: typing.Optional[builtins.int] = ...,
        dynamic_partition_metadata: typing.Optional[global___DynamicPartitionMetadata] = ...,
        partial_update: typing.Optional[builtins.bool] = ...,
        apex_info: typing.Optional[typing.Iterable[global___ApexInfo]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["block_size",b"block_size","dynamic_partition_metadata",b"dynamic_partition_metadata","max_timestamp",b"max_timestamp","minor_version",b"minor_version","partial_update",b"partial_update","signatures_offset",b"signatures_offset","signatures_size",b"signatures_size"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["apex_info",b"apex_info","block_size",b"block_size","dynamic_partition_metadata",b"dynamic_partition_metadata","max_timestamp",b"max_timestamp","minor_version",b"minor_version","partial_update",b"partial_update","partitions",b"partitions","signatures_offset",b"signatures_offset","signatures_size",b"signatures_size"]) -> None: ...
global___DeltaArchiveManifest = DeltaArchiveManifest
